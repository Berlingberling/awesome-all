# awesome_deep_learning_interpretability
深度学习近年来关于模型解释性的相关论文。

按引用次数排序可见[引用排序](./sort_cite.md)

159篇论文pdf(有2篇需要上scihub找)上传到[腾讯微云](https://share.weiyun.com/5ddB0EQ)。

不定期更新。

|Year|Publication|Paper|Citation|code|
|:---:|:---:|:---:|:---:|:---:|
|2020|CVPR|[Explaining Knowledge Distillation by Quantifying the Knowledge](https://arxiv.org/pdf/2003.03622.pdf)|0|
|2020|ICLR|Knowledge Isomorphism between Neural Networks|0|
|2020|ICLR|Interpretable Complex-Valued Neural Networks for Privacy Protection|2|
|2019|AI|Explanation in artificial intelligence: Insights from the social sciences|495|
|2019|NMI|Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead|142|
|2019|NeurIPS|This looks like that: deep learning for interpretable image recognition|49|[Pytorch](https://github.com/cfchen-duke/ProtoPNet)|
|2019|NeurIPS|A benchmark for interpretability methods in deep neural networks（同arxiv:1806.10758）|6|
|2019|NeurIPS|Full-gradient representation for neural network visualization|3|
|2019|NeurIPS|On the (In) fidelity and Sensitivity of Explanations|3|
|2019|NeurIPS|Towards Automatic Concept-based Explanations|6|[Tensorflow](https://github.com/amiratag/ACE)|
|2019|NeurIPS|CXPlain: Causal explanations for model interpretation under uncertainty|3|
|2019|CVPR|Interpreting CNNs via Decision Trees|65|
|2019|CVPR|From Recognition to Cognition: Visual Commonsense Reasoning|63|[Pytorch](https://github.com/rowanz/r2c)|
|2019|CVPR|Attention branch network: Learning of attention mechanism for visual explanation|22|
|2019|CVPR|Interpretable and fine-grained visual explanations for convolutional neural networks|9|
|2019|CVPR|Learning to Explain with Complemental Examples|9|
|2019|CVPR|Revealing Scenes by Inverting Structure from Motion Reconstructions|8|[Tensorflow](https://github.com/francescopittaluga/invsfm)|
|2019|CVPR|Multimodal Explanations by Predicting Counterfactuality in Videos|3|
|2019|CVPR|Visualizing the Resilience of Deep Convolutional Network Interpretations|1|
|2019|ICCV|U-CAM: Visual Explanation using Uncertainty based Class Activation Maps|9|
|2019|ICCV|Towards Interpretable Face Recognition|7|
|2019|ICCV|Taking a HINT: Leveraging Explanations to Make Vision and Language Models More Grounded|10|
|2019|ICCV|Understanding Deep Networks via Extremal Perturbations and Smooth Masks|10|[Pytorch](https://github.com/facebookresearch/TorchRay)|
|2019|ICCV|Explaining Neural Networks Semantically and Quantitatively|4|
|2019|ICLR|Hierarchical interpretations for neural network predictions|22|[Pytorch](https://github.com/csinva/hierarchical-dnn-interpretations)|
|2019|ICLR|How Important Is a Neuron?|10|
|2019|ICLR|Visual Explanation by Interpretation: Improving Visual Feedback Capabilities of Deep Neural Networks|9|
|2018|ICML|Extracting Automata from Recurrent Neural Networks Using Queries and Counterexamples|60|[Pytorch](https://github.com/tech-srl/lstar_extraction)|
|2019|ICML|Towards A Deep and Unified Understanding of Deep Neural Models in NLP|10|[Pytorch](https://github.com/icml2019paper2428/Towards-A-Deep-and-Unified-Understanding-of-Deep-Neural-Models-in-NLP)|
|2019|ICAIS|Interpreting black box predictions using fisher kernels|14|
|2019|ACMFAT|Explaining explanations in AI|78|
|2019|AAAI|Interpretation of neural networks is fragile|87|[Tensorflow](https://github.com/amiratag/InterpretationFragility)|
|2019|AAAI|Classifier-agnostic saliency map extraction|7|
|2019|AAAI|Can You Explain That? Lucid Explanations Help Human-AI Collaborative Image Retrieval|1|
|2019|AAAIW|Unsupervised Learning of Neural Networks to Explain Neural Networks|9|
|2019|AAAIW|Network Transplanting|4|
|2019|CSUR|A Survey of Methods for Explaining Black Box Models|455|
|2019|JVCIR|Interpretable convolutional neural networks via feedforward design|28|[Keras](https://github.com/davidsonic/Interpretable_CNNs_via_Feedforward_Design)|
|2019|ExplainAI|The (Un)reliability of saliency methods(scihub)|115|
|2019|ACL|Attention is not Explanation|75|
|2019|arxiv|Attention Interpretability Across NLP Tasks|5|
|2019|arxiv|Interpretable CNNs|2|
|2018|ICLR|Towards better understanding of gradient-based attribution methods for deep neural networks|151|
|2018|ICLR|Learning how to explain neural networks: PatternNet and PatternAttribution|111|
|2018|ICLR|On the importance of single directions for generalization|102|[Pytorch](https://github.com/1Konny/class_selectivity_index)|
|2018|ICLR|Detecting statistical interactions from neural network weights|42|[Pytorch](https://github.com/mtsang/neural-interaction-detection)|
|2018|ICLR|Interpretable counting for visual question answering|25|[Pytorch](https://github.com/sanyam5/irlc-vqa-counting)|
|2018|CVPR|Interpretable Convolutional Neural Networks|190|
|2018|CVPR|Tell me where to look: Guided attention inference network|98|[Chainer](https://github.com/alokwhitewolf/Guided-Attention-Inference-Network)|
|2018|CVPR|Multimodal Explanations: Justifying Decisions and Pointing to the Evidence|94|[Caffe](https://github.com/Seth-Park/MultimodalExplanations)|
|2018|CVPR|Transparency by design: Closing the gap between performance and interpretability in visual reasoning|66|[Pytorch](https://github.com/davidmascharka/tbd-nets)|
|2018|CVPR|Net2vec: Quantifying and explaining how concepts are encoded by filters in deep neural networks|45|
|2018|CVPR|What have we learned from deep representations for action recognition?|23|
|2018|CVPR|Learning to Act Properly: Predicting and Explaining Affordances from Images|21|
|2018|CVPR|Teaching Categories to Human Learners with Visual Explanations|17|[Pytorch](https://github.com/macaodha/explain_teach)|
|2018|CVPR|What do Deep Networks Like to See?|12|
|2018|CVPR|Interpret Neural Networks by Identifying Critical Data Routing Paths|10|[Tensorflow](https://github.com/lidongyue12138/CriticalPathPruning)|
|2018|ECCV|Deep clustering for unsupervised learning of visual features|245|[Pytorch](https://github.com/asanakoy/deep_clustering)|
|2018|ECCV|Explainable neural computation via stack neural module networks|43|[Tensorflow](https://github.com/ronghanghu/snmn)|
|2018|ECCV|Grounding visual explanations|34|
|2018|ECCV|Textual explanations for self-driving vehicles|42|
|2018|ECCV|Interpretable basis decomposition for visual explanation|32|[Pytorch](https://github.com/CSAILVision/IBD)|
|2018|ECCV|Convnets and imagenet beyond accuracy: Understanding mistakes and uncovering biases|20|
|2018|ECCV|Vqa-e: Explaining, elaborating, and enhancing your answers for visual questions|13|
|2018|ECCV|Choose Your Neuron: Incorporating Domain Knowledge through Neuron-Importance|13|[Pytorch](https://github.com/ramprs/neuron-importance-zsl)|
|2018|ECCV|Diverse feature visualizations reveal invariances in early layers of deep neural networks|7|[Tensorflow](https://github.com/sacadena/diverse_feature_vis)|
|2018|ECCV|ExplainGAN: Model Explanation via Decision Boundary Crossing Transformations|1|
|2018|ICML|Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)|148|[Tensorflow](https://github.com/fursovia/tcav_nlp)|
|2018|ICML|Learning to explain: An information-theoretic perspective on model interpretation|88|
|2018|ACL|Did the Model Understand the Question?|39|[Tensorflow](https://github.com/pramodkaushik/acl18_results)|
|2018|FITEE|Visual interpretability for deep learning: a survey|183|
|2018|NeurIPS|Sanity Checks for Saliency Maps|165|
|2018|NeurIPS|Explanations based on the missing: Towards contrastive explanations with pertinent negatives|47|[Tensorflow](https://github.com/IBM/Contrastive-Explanation-Method)|
|2018|NeurIPS|Towards robust interpretability with self-explaining neural networks|94|[Pytorch](https://github.com/raj-shah/senn)|
|2018|NeurIPS|Attacks meet interpretability: Attribute-steered detection of adversarial samples|35|
|2018|NeurIPS Workshop|Interpretable Convolutional Filters with SincNet|27|
|2018|NeurIPS|DeepPINK: reproducible feature selection in deep neural networks|21|[Keras](https://github.com/younglululu/DeepPINK)|
|2018|NeurIPS|Representer point selection for explaining deep neural networks|15|[Tensorflow](https://github.com/chihkuanyeh/Representer_Point_Selection)|
|2018|AAAI|Anchors: High-precision model-agnostic explanations|269|
|2018|AAAI|Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients|138|[Tensorflow](https://github.com/dtak/adversarial-robustness-public)|
|2018|AAAI|Deep learning for case-based reasoning through prototypes: A neural network that explains its predictions|80|[Tensorflow](https://github.com/OscarcarLi/PrototypeDL)|
|2018|AAAI|Interpreting CNN Knowledge via an Explanatory Graph|67|[Matlab](https://github.com/zqs1022/explanatoryGraph)|
|2018|AAAI|Examining CNN Representations with respect to Dataset Bias|32|
|2018|WACV|Grad-cam++: Generalized gradient-based visual explanations for deep convolutional networks|113|
|2018|IJCV|Top-down neural attention by excitation backprop|285|
|2018|TPAMI|Interpreting deep visual representations via network dissection|69|
|2018|DSP|Methods for interpreting and understanding deep neural networks(scihub)|568|
|2018|Access|Peeking inside the black-box: A survey on Explainable Artificial Intelligence (XAI)|228|
|2018|JAIR|Learning Explanatory Rules from Noisy Data|111|[Tensorflow](https://github.com/ai-systems/DILP-Core)|
|2018|MIPRO|Explainable artificial intelligence: A survey|82|
|2018|AIES|Detecting Bias in Black-Box Models Using Transparent Model Distillation|28|
|2018|BMVC|Rise: Randomized input sampling for explanation of black-box models|46|
|2018|arxiv|Manipulating and measuring model interpretability|101|
|2018|arxiv|How convolutional neural network see the world-A survey of convolutional neural network visualization methods|33|
|2018|arxiv|Revisiting the importance of individual units in cnns via ablation|31|
|2018|arxiv|Computationally Efficient Measures of Internal Neuron Importance|1|
|2017|ICML|Understanding Black-box Predictions via Influence Functions|610|[Pytorch](https://github.com/nimarb/pytorch_influence_functions)|
|2017|ICML|Axiomatic attribution for deep networks|561|[Keras](https://github.com/hiranumn/IntegratedGradients)|
|2017|ICML|Learning Important Features Through Propagating Activation Differences|489|
|2017|ICLR|Visualizing deep neural network decisions: Prediction difference analysis|231|[Caffe](https://github.com/lmzintgraf/DeepVis-PredDiff)|
|2017|ICLR|Exploring LOTS in Deep Neural Networks|26|
|2017|NeurIPS|A Unified Approach to Interpreting Model Predictions|894|
|2017|NeurIPS|Real time image saliency for black box classifiers|128|[Pytorch](https://github.com/karanchahal/SaliencyMapper)|
|2017|NeurIPS|SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability|122|
|2017|CVPR|Mining Object Parts from CNNs via Active Question-Answering|16|
|2017|CVPR|Network dissection: Quantifying interpretability of deep visual representations|435|
|2017|CVPR|Improving Interpretability of Deep Neural Networks with Semantic Information|52|
|2017|CVPR|MDNet: A Semantically and Visually Interpretable Medical Image Diagnosis Network|100|[Torch](https://github.com/zizhaozhang/mdnet-cvpr2017)|
|2017|CVPR|Interpretable 3d human action analysis with temporal convolutional networks|133|
|2017|CVPR|Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering|454|
|2017|CVPR|Knowing when to look: Adaptive attention via a visual sentinel for image captioning|525|[Torch](https://github.com/jiasenlu/AdaptiveAttention)|
|2017|ICCV|Grad-cam: Visual explanations from deep networks via gradient-based localization|1755|[Pytorch](https://github.com/leftthomas/GradCAM)|
|2017|ICCV|Interpretable Explanations of Black Boxes by Meaningful Perturbation|339|[Pytorch](https://github.com/jacobgil/pytorch-explain-black-box)|
|2017|ICCV|Interpretable Learning for Self-Driving Cars by Visualizing Causal Attention|96|
|2017|ICCV|Understanding and comparing deep neural networks for age and gender classification|48|
|2017|ICCV|Learning to disambiguate by asking discriminative questions|11|
|2017|IJCAI|Right for the right reasons: Training differentiable models by constraining their explanations|118|
|2017|IJCAI|Understanding and improving convolutional neural networks via concatenated rectified linear units|237|[Caffe](https://github.com/chakkritte/CReLU)|
|2017|AAAI|Growing Interpretable Part Graphs on ConvNets via Multi-Shot Learning|28|[Matlab](https://github.com/zqs1022/partGraphForCNN)|
|2017|ACL|Visualizing and Understanding Neural Machine Translation|63|
|2017|EMNLP|A causal framework for explaining the predictions of black-box sequence-to-sequence models|68|
|2017|CVPR Workshop|Looking under the hood: Deep neural network visualization to interpret whole-slide image analysis outcomes for colorectal polyps|16|
|2017|survey|Interpretability of deep learning models: a survey of results|63|
|2017|arxiv|SmoothGrad: removing noise by adding noise|265|
|2017|arxiv|Interpretable & explorable approximations of black box models|91|
|2017|arxiv|Distilling a neural network into a soft decision tree|153|[Pytorch](https://github.com/kimhc6028/soft-decision-tree)|
|2017|arxiv|Towards interpretable deep neural networks by leveraging adversarial examples|52|
|2017|arxiv|Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models|294|
|2017|arxiv|Contextual Explanation Networks|32|[Pytorch](https://github.com/alshedivat/cen)|
|2017|arxiv|Challenges for transparency|74|
|2017|ACMSOPP|Deepxplore: Automated whitebox testing of deep learning systems|364|
|2017|CEURW|What does explainable AI really mean? A new conceptualization of perspectives|83|
|2017|TVCG|ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models|131|
|2016|NeurIPS|Synthesizing the preferred inputs for neurons in neural networks via deep generator networks|277|[Caffe](https://github.com/Evolving-AI-Lab/synthesizing)|
|2016|NeurIPS|Understanding the effective receptive field in deep convolutional neural networks|358|
|2016|CVPR|Inverting Visual Representations with Convolutional Networks|295|
|2016|CVPR|Visualizing and Understanding Deep Texture Representations|89|
|2016|CVPR|Analyzing Classifiers: Fisher Vectors and Deep Neural Networks|94|
|2016|ECCV|Generating Visual Explanations|256|[Caffe](https://github.com/LisaAnne/ECCV2016)|
|2016|ECCV|Design of kernels in convolutional neural networks for image classification|13|
|2016|ICML|Understanding and improving convolutional neural networks via concatenated rectified linear units|237|
|2016|ICML|Visualizing and comparing AlexNet and VGG using deconvolutional layers|31|
|2016|EMNLP|Rationalizing Neural Predictions|276|[Pytorch](https://github.com/zhaopku/Rationale-Torch)|
|2016|IJCV|Visualizing deep convolutional neural networks using natural pre-images|243|[Matlab](https://github.com/aravindhm/nnpreimage)|
|2016|IJCV|Visualizing Object Detection Features|23|[Caffe](https://github.com/cvondrick/ihog)|
|2016|KDD|Why should i trust you?: Explaining the predictions of any classifier|2766|
|2016|TVCG|Visualizing the hidden activity of artificial neural networks|143|
|2016|TVCG|Towards better analysis of deep convolutional neural networks|210|
|2016|NAACL|Visualizing and understanding neural models in nlp|292|[Torch](https://github.com/jiweil/Visualizing-and-Understanding-Neural-Models-in-NLP)|
|2016|arxiv|Understanding neural networks through representation erasure|150|
|2016|arxiv|Grad-CAM: Why did you say that?|106|
|2016|arxiv|Investigating the influence of noise and distractors on the interpretation of neural networks|29|
|2016|arxiv|Attentive Explanations: Justifying Decisions and Pointing to the Evidence|46|
|2016|arxiv|The Mythos of Model Interpretability|1108|
|2016|arxiv|Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks|140|
|2015|ICLR|Striving for Simplicity: The All Convolutional Net|1998|[Pytorch](https://github.com/StefOe/all-conv-pytorch)|
|2015|CVPR|Understanding deep image representations by inverting them|1009|[Matlab](https://github.com/aravindhm/deep-goggle)|
|2015|ICCV|Understanding deep features with computer-generated imagery|100|[Caffe](https://github.com/mathieuaubry/features_analysis)|
|2015|ICML Workshop|Understanding Neural Networks Through Deep Visualization|1072|[Tensorflow](https://github.com/jiye-ML/Visualizing-and-Understanding-Convolutional-Networks)|
|2015|AAS|Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model|331|
|2014|ECCV|Visualizing and Understanding Convolutional Networks|8856|[Pytorch](https://github.com/huybery/VisualizingCNN)|
|2014|ICLR|Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps|2277|[Pytorch](https://github.com/huanghao-code/VisCNN_ICLR_2014_Saliency)|

 
+ [ ] 论文talk
